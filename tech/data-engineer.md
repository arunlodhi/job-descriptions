## Position: Data Engineer

### Job Summary:

As a Data Engineer, you will be at the forefront of our data infrastructure and analytics initiatives. Your expertise in designing, building, and maintaining data pipelines, including those on Kubernetes, will drive the successful extraction, transformation, and loading (ETL) of data from various sources. You will collaborate with data scientists, analysts, and other stakeholders to ensure the availability and accuracy of data for analysis and decision-making.

### Key Responsibilities:

- **Data Pipeline Development:** Design, implement, and maintain data pipelines, including Kubernetes-native solutions, to extract, transform, and load data from diverse sources into our data warehouse or data lake.

- **Data Modeling:** Develop and maintain data models, including Kubernetes configuration, to optimize data storage and retrieval for analytical purposes.

- **Data Quality and Validation:** Implement data quality checks and validation processes to ensure data accuracy and integrity, including Kubernetes-specific checks.

- **Performance Optimization:** Optimize data pipelines and data storage, leveraging Kubernetes capabilities, for efficient and cost-effective data processing.

- **Data Integration:** Integrate data from various systems and third-party sources to create a unified view of the data, utilizing Kubernetes-native data integration tools.

- **Data Security and Compliance:** Implement data security measures and ensure compliance with data privacy regulations, considering Kubernetes security practices.

- **Automation:** Develop automation scripts and tools to streamline manual processes and improve overall efficiency in managing data on Kubernetes.

- **Documentation:** Maintain detailed documentation of data pipelines, data models, and processes, including Kubernetes configuration details.

### Key Requirements:

- Proven experience as a Data Engineer or similar role with a focus on data pipelines and ETL processes, including Kubernetes-based solutions.
- In-depth knowledge of data warehousing concepts, data modeling, and database systems, with Kubernetes-specific considerations.
- Proficiency in Kubernetes-native data processing frameworks like Apache Spark on Kubernetes or Apache Flink.
- Experience with Kubernetes networking and storage concepts for data processing workloads.
- Familiarity with Kubernetes-based data integration and ETL tools.
- Understanding of big data technologies and distributed computing frameworks.
- Knowledge of programming languages like Python, Scala, or Java for data manipulation and scripting, including Kubernetes configuration.
- Strong SQL skills and understanding of database performance optimization, including Kubernetes database integration.
- Excellent problem-solving and analytical skills.
- Effective communication and teamwork abilities.
